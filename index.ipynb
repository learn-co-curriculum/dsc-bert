{"cells":[{"cell_type":"markdown","metadata":{},"source":["# BERT - Pair Programming"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Introduction\n","\n","**BERT (Bidirectional Encoder Representation from Transformer)** is a linguistic embedding model published by Google. It is a context-based model, unlike other embedding models such as word2vec, which are context-free. The context-sensitive nature of BERT was built upon a dataset of 3.3 billion words, in particular approximately 2.5 billion from Wikipedia and the balance from Google's [BookCorpus](https://www.english-corpora.org/googlebooks/#)."]},{"cell_type":"markdown","metadata":{},"source":["## Objectives\n","\n","You will be able to: \n","\n","* To understand how to implement BERT in Python\n","* To apply BERT to NLP\n","* Understand the possibility of bias when working with BERT\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Some details of the BERT Model\n","\n","Based on our previous discussion of the transformer, we can see where the terms \"encoder representation from transformer\" come from. But what about \"Bidirectional?\" Bidirectional simply mean the encoder can read the sentence in both directions, e.g. both Cogito ergo sum to I think therefore I am and vice versa.\n","\n","BERT has three main hyperparameters\n","* $L$ is the number of encoder layers\n","* $A$ is the number of attention heads\n","* $H$ is the number of hidden units\n","\n","The model also comes in some pre-specified configurations, and here are the two standard ones\n","* BERT-base: $L=12$, $A=12$, $H=768$\n","* BERT-large: $L=42$, $A=16$, $H=1,024$\n","\n","In particular, we'll be using BERT to help discover the missing word in a sentence. BERT can also be used for translation and Next Sentence Prediction (NSP) as well as a myriad of other applications."]},{"cell_type":"markdown","metadata":{},"source":["## Using BERT\n","\n","We'll need to use the [Python library `transformers`](https://huggingface.co/transformers/v3.0.2/index.html). The `transformers` library provides general-purpose architectures such as BERT for NLP, with over 32 pretrained models in more than 100 languages.\n","\n","The intent is to run this exercise in SaturnCloud since there can be some issues when trying to [install `transformers` locally](https://huggingface.co/docs/transformers/installation)."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Import the german libraries\n","from transformers import pipeline"]},{"cell_type":"markdown","metadata":{},"source":["## Masking with BERT\n","\n","The model ```bert-base-uncased``` is one of the pretrained BERT models and it has 110 million parameters. [Details of this model can be found on Hugging Face](https://huggingface.co/bert-base-uncased). We'll be using ```bert-base-uncased``` for masking.\n","\n","You may get a comment from BERT regarding weights of ```bert-base-uncased```, but this is nothing to worry about for our purposes."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["# Define our function unmasker\n","unmasker = pipeline('fill-mask', model='bert-base-uncased')"]},{"cell_type":"markdown","metadata":{},"source":["Let's try a sentence and see how BERT does."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["[{'score': 0.3182407021522522,\n","  'token': 2064,\n","  'token_str': 'can',\n","  'sequence': 'artificial intelligence can take over the world.'},\n"," {'score': 0.18299739062786102,\n","  'token': 2097,\n","  'token_str': 'will',\n","  'sequence': 'artificial intelligence will take over the world.'},\n"," {'score': 0.05600160360336304,\n","  'token': 2000,\n","  'token_str': 'to',\n","  'sequence': 'artificial intelligence to take over the world.'},\n"," {'score': 0.04519487917423248,\n","  'token': 2015,\n","  'token_str': '##s',\n","  'sequence': 'artificial intelligences take over the world.'},\n"," {'score': 0.04515313729643822,\n","  'token': 2052,\n","  'token_str': 'would',\n","  'sequence': 'artificial intelligence would take over the world.'}]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# [MASK] goes in the place you want BERT to predict the correct word\n","unmasker(\"Artificial Intelligence [MASK] take over the world.\")"]},{"cell_type":"markdown","metadata":{},"source":["The top five possibilities are shown. Further, the token string with the highest score is the one with the highest probability of being correct according to BERT. In this example, it is \"can\" as in \"artificial intelligence can take over the world\" at a 32% probability.\n","\n","On supposes we should be happy that \"can\" has a higher probability than \"will.\""]},{"cell_type":"markdown","metadata":{},"source":["In the output, ```token``` refers to the position of the masked token in the list that is generated from the transformer. For our purposes, we don't have to worry about that, but only ```score``` and ```token_str``` with the corresponding ```sequence```."]},{"cell_type":"markdown","metadata":{},"source":["### Task 1: Masking Twice\n","\n","What happens if one used ```[MASK]``` two times in a sentence?\n","\n","For example, run the following in the code block below and interpret the results.\n","\n","\n","```\n","unmasker(\"Artificial Intelligence [MASK] take over the [MASK].\")\n","```\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["[[{'score': 0.20802287757396698,\n","   'token': 2064,\n","   'token_str': 'can',\n","   'sequence': '[CLS] artificial intelligence can take over the [MASK]. [SEP]'},\n","  {'score': 0.11164189875125885,\n","   'token': 2097,\n","   'token_str': 'will',\n","   'sequence': '[CLS] artificial intelligence will take over the [MASK]. [SEP]'},\n","  {'score': 0.04858846962451935,\n","   'token': 2052,\n","   'token_str': 'would',\n","   'sequence': '[CLS] artificial intelligence would take over the [MASK]. [SEP]'},\n","  {'score': 0.04662353917956352,\n","   'token': 3001,\n","   'token_str': 'systems',\n","   'sequence': '[CLS] artificial intelligence systems take over the [MASK]. [SEP]'},\n","  {'score': 0.03878749534487724,\n","   'token': 2000,\n","   'token_str': 'to',\n","   'sequence': '[CLS] artificial intelligence to take over the [MASK]. [SEP]'}],\n"," [{'score': 0.1323976367712021,\n","   'token': 2088,\n","   'token_str': 'world',\n","   'sequence': '[CLS] artificial intelligence [MASK] take over the world. [SEP]'},\n","  {'score': 0.1070786789059639,\n","   'token': 2208,\n","   'token_str': 'game',\n","   'sequence': '[CLS] artificial intelligence [MASK] take over the game. [SEP]'},\n","  {'score': 0.025642095133662224,\n","   'token': 5304,\n","   'token_str': 'universe',\n","   'sequence': '[CLS] artificial intelligence [MASK] take over the universe. [SEP]'},\n","  {'score': 0.025563597679138184,\n","   'token': 2291,\n","   'token_str': 'system',\n","   'sequence': '[CLS] artificial intelligence [MASK] take over the system. [SEP]'},\n","  {'score': 0.017930585891008377,\n","   'token': 2565,\n","   'token_str': 'program',\n","   'sequence': '[CLS] artificial intelligence [MASK] take over the program. [SEP]'}]]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Using [MASK] twice\n","unmasker(\"Artificial Intelligence [MASK] take over the [MASK].\")"]},{"cell_type":"markdown","metadata":{},"source":["*Explain and interpret the \"double-mask\" here.*"]},{"cell_type":"markdown","metadata":{},"source":["### Task 2: Using unmasker\n","\n","Use unmasker on three other sentences. At least one of them should be a \"double-mask.\" Explain and interpret each one."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["[{'score': 0.21098032593727112,\n","  'token': 2338,\n","  'token_str': 'book',\n","  'sequence': 'she carefully placed the book on the top shelf.'},\n"," {'score': 0.07043339312076569,\n","  'token': 2808,\n","  'token_str': 'books',\n","  'sequence': 'she carefully placed the books on the top shelf.'},\n"," {'score': 0.06375743448734283,\n","  'token': 3482,\n","  'token_str': 'box',\n","  'sequence': 'she carefully placed the box on the top shelf.'},\n"," {'score': 0.0198493804782629,\n","  'token': 3661,\n","  'token_str': 'letter',\n","  'sequence': 'she carefully placed the letter on the top shelf.'},\n"," {'score': 0.016116991639137268,\n","  'token': 5835,\n","  'token_str': 'bottle',\n","  'sequence': 'she carefully placed the bottle on the top shelf.'}]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Example 1\n","unmasker(\"She carefully placed the [MASK] on the top shelf.\")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["[{'score': 0.3824915885925293,\n","  'token': 12760,\n","  'token_str': 'ingredients',\n","  'sequence': \"i need to buy some ingredients for the recipe i'm cooking.\"},\n"," {'score': 0.037283189594745636,\n","  'token': 2051,\n","  'token_str': 'time',\n","  'sequence': \"i need to buy some time for the recipe i'm cooking.\"},\n"," {'score': 0.022978467866778374,\n","  'token': 12136,\n","  'token_str': 'butter',\n","  'sequence': \"i need to buy some butter for the recipe i'm cooking.\"},\n"," {'score': 0.019512977451086044,\n","  'token': 13724,\n","  'token_str': 'flour',\n","  'sequence': \"i need to buy some flour for the recipe i'm cooking.\"},\n"," {'score': 0.019034557044506073,\n","  'token': 2833,\n","  'token_str': 'food',\n","  'sequence': \"i need to buy some food for the recipe i'm cooking.\"}]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# Example 2\n","unmasker(\"I need to buy some [MASK] for the recipe I'm cooking.\")"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["[[{'score': 0.40234193205833435,\n","   'token': 2493,\n","   'token_str': 'students',\n","   'sequence': '[CLS] the students were eager to learn about the [MASK] of ancient civilizations. [SEP]'},\n","  {'score': 0.05683496966958046,\n","   'token': 2336,\n","   'token_str': 'children',\n","   'sequence': '[CLS] the children were eager to learn about the [MASK] of ancient civilizations. [SEP]'},\n","  {'score': 0.04617047309875488,\n","   'token': 5731,\n","   'token_str': 'visitors',\n","   'sequence': '[CLS] the visitors were eager to learn about the [MASK] of ancient civilizations. [SEP]'},\n","  {'score': 0.02000384032726288,\n","   'token': 9045,\n","   'token_str': 'tourists',\n","   'sequence': '[CLS] the tourists were eager to learn about the [MASK] of ancient civilizations. [SEP]'},\n","  {'score': 0.01869625225663185,\n","   'token': 6368,\n","   'token_str': 'guests',\n","   'sequence': '[CLS] the guests were eager to learn about the [MASK] of ancient civilizations. [SEP]'}],\n"," [{'score': 0.49981510639190674,\n","   'token': 2381,\n","   'token_str': 'history',\n","   'sequence': '[CLS] the [MASK] were eager to learn about the history of ancient civilizations. [SEP]'},\n","  {'score': 0.04050268977880478,\n","   'token': 7321,\n","   'token_str': 'origins',\n","   'sequence': '[CLS] the [MASK] were eager to learn about the origins of ancient civilizations. [SEP]'},\n","  {'score': 0.032915644347667694,\n","   'token': 24884,\n","   'token_str': 'workings',\n","   'sequence': '[CLS] the [MASK] were eager to learn about the workings of ancient civilizations. [SEP]'},\n","  {'score': 0.026313837617635727,\n","   'token': 7800,\n","   'token_str': 'secrets',\n","   'sequence': '[CLS] the [MASK] were eager to learn about the secrets of ancient civilizations. [SEP]'},\n","  {'score': 0.021959422156214714,\n","   'token': 4294,\n","   'token_str': 'architecture',\n","   'sequence': '[CLS] the [MASK] were eager to learn about the architecture of ancient civilizations. [SEP]'}]]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Example 3\n","unmasker(\"The [MASK] were eager to learn about the [MASK] of ancient civilizations.\")"]},{"cell_type":"markdown","metadata":{},"source":["### Literary Interlude\n","\n","How does ```unmasker``` perform with a quote from literature or other notable work?"]},{"cell_type":"markdown","metadata":{},"source":["Let's look first a \"To be, or not to be, that is the question\" from William Shakespeare's *Hamlet* (Act 3, Scene 1)."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["[{'score': 0.1824198216199875,\n","  'token': 3160,\n","  'token_str': 'question',\n","  'sequence': 'to be, or not to be, that is the question :'},\n"," {'score': 0.122404083609581,\n","  'token': 3437,\n","  'token_str': 'answer',\n","  'sequence': 'to be, or not to be, that is the answer :'},\n"," {'score': 0.09915042668581009,\n","  'token': 2553,\n","  'token_str': 'case',\n","  'sequence': 'to be, or not to be, that is the case :'},\n"," {'score': 0.03269161656498909,\n","  'token': 2168,\n","  'token_str': 'same',\n","  'sequence': 'to be, or not to be, that is the same :'},\n"," {'score': 0.02776072546839714,\n","  'token': 2518,\n","  'token_str': 'thing',\n","  'sequence': 'to be, or not to be, that is the thing :'}]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# Let's mask \"question\"\n","unmasker(\"To be, or not to be, that is the [MASK]:\")"]},{"cell_type":"markdown","metadata":{},"source":["We can see that the highest probability does give us the correct answer.\n","\n","Let's look at another one.\n","\n","The opening line of James Joyce's Ulysses is “Stately, plump Buck Mulligan came from the stairhead, bearing a bowl of lather on which a mirror and a razor lay crossed.”"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["[{'score': 0.22326043248176575,\n","  'token': 2214,\n","  'token_str': 'old',\n","  'sequence': 'stately, old buck mulligan came from the stairhead, bearing a bowl of lather on which a mirror and a razor lay crossed.'},\n"," {'score': 0.10754996538162231,\n","  'token': 1996,\n","  'token_str': 'the',\n","  'sequence': 'stately, the buck mulligan came from the stairhead, bearing a bowl of lather on which a mirror and a razor lay crossed.'},\n"," {'score': 0.09361004829406738,\n","  'token': 2402,\n","  'token_str': 'young',\n","  'sequence': 'stately, young buck mulligan came from the stairhead, bearing a bowl of lather on which a mirror and a razor lay crossed.'},\n"," {'score': 0.07783853262662888,\n","  'token': 3335,\n","  'token_str': 'miss',\n","  'sequence': 'stately, miss buck mulligan came from the stairhead, bearing a bowl of lather on which a mirror and a razor lay crossed.'},\n"," {'score': 0.0626087561249733,\n","  'token': 2909,\n","  'token_str': 'sir',\n","  'sequence': 'stately, sir buck mulligan came from the stairhead, bearing a bowl of lather on which a mirror and a razor lay crossed.'}]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Let's mask \"plump\"\n","unmasker(\"Stately, [MASK] Buck Mulligan came from the stairhead, bearing a bowl of lather on which a mirror and a razor lay crossed.\")"]},{"cell_type":"markdown","metadata":{},"source":["We see that the actual word- \"plump\"- did not make the top 5.\n","\n","Now let's unmask \"plump\" and mask \"lather.\""]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["[{'score': 0.16707313060760498,\n","  'token': 2300,\n","  'token_str': 'water',\n","  'sequence': 'stately, plump buck mulligan came from the stairhead, bearing a bowl of water on which a mirror and a razor lay crossed.'},\n"," {'score': 0.07017775624990463,\n","  'token': 8416,\n","  'token_str': 'cloth',\n","  'sequence': 'stately, plump buck mulligan came from the stairhead, bearing a bowl of cloth on which a mirror and a razor lay crossed.'},\n"," {'score': 0.05842616781592369,\n","  'token': 7815,\n","  'token_str': 'soap',\n","  'sequence': 'stately, plump buck mulligan came from the stairhead, bearing a bowl of soap on which a mirror and a razor lay crossed.'},\n"," {'score': 0.052040643990039825,\n","  'token': 20717,\n","  'token_str': 'stew',\n","  'sequence': 'stately, plump buck mulligan came from the stairhead, bearing a bowl of stew on which a mirror and a razor lay crossed.'},\n"," {'score': 0.047007400542497635,\n","  'token': 4511,\n","  'token_str': 'wine',\n","  'sequence': 'stately, plump buck mulligan came from the stairhead, bearing a bowl of wine on which a mirror and a razor lay crossed.'}]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# Let's mask \"lather\"\n","unmasker(\"Stately, plump Buck Mulligan came from the stairhead, bearing a bowl of [MASK] on which a mirror and a razor lay crossed.\")"]},{"cell_type":"markdown","metadata":{},"source":["While \"lather\" is not picked, the 3rd choice of the model is \"soap,\" which is a synonym."]},{"cell_type":"markdown","metadata":{},"source":["### Task 3: A quote from literature or other notable work\n","\n","Now it is your turn.\n","\n","Find a quote from literature or other notable work such as from a philosophical or religious text and make sure to state where the quote is from.\n","\n","Mask at least two different words and see how BERT performs."]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["[[{'score': 0.07329743355512619,\n","   'token': 2009,\n","   'token_str': 'it',\n","   'sequence': '[CLS] reserving it is a matter of infinite [MASK]. [SEP]'},\n","  {'score': 0.0404452309012413,\n","   'token': 2068,\n","   'token_str': 'them',\n","   'sequence': '[CLS] reserving them is a matter of infinite [MASK]. [SEP]'},\n","  {'score': 0.026655122637748718,\n","   'token': 2028,\n","   'token_str': 'one',\n","   'sequence': '[CLS] reserving one is a matter of infinite [MASK]. [SEP]'},\n","  {'score': 0.025434434413909912,\n","   'token': 2242,\n","   'token_str': 'something',\n","   'sequence': '[CLS] reserving something is a matter of infinite [MASK]. [SEP]'},\n","  {'score': 0.022833487018942833,\n","   'token': 17213,\n","   'token_str': 'forgiveness',\n","   'sequence': '[CLS] reserving forgiveness is a matter of infinite [MASK]. [SEP]'}],\n"," [{'score': 0.13453197479248047,\n","   'token': 2051,\n","   'token_str': 'time',\n","   'sequence': '[CLS] reserving [MASK] is a matter of infinite time. [SEP]'},\n","  {'score': 0.07745572179555893,\n","   'token': 11752,\n","   'token_str': 'patience',\n","   'sequence': '[CLS] reserving [MASK] is a matter of infinite patience. [SEP]'},\n","  {'score': 0.03692149743437767,\n","   'token': 3947,\n","   'token_str': 'effort',\n","   'sequence': '[CLS] reserving [MASK] is a matter of infinite effort. [SEP]'},\n","  {'score': 0.034030262380838394,\n","   'token': 2373,\n","   'token_str': 'power',\n","   'sequence': '[CLS] reserving [MASK] is a matter of infinite power. [SEP]'},\n","  {'score': 0.03216096758842468,\n","   'token': 5197,\n","   'token_str': 'importance',\n","   'sequence': '[CLS] reserving [MASK] is a matter of infinite importance. [SEP]'}]]"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# Sample: \"Reserving judgments is a matter of infinite hope.\" - F. Scott Fitzgerald, The Great Gatsby\n","unmasker(\"Reserving [MASK] is a matter of infinite [MASK].\")"]},{"cell_type":"markdown","metadata":{},"source":["### Task 4: Bias in the model\n","\n","Run the following two code cells."]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["[{'score': 0.09747529029846191,\n","  'token': 10533,\n","  'token_str': 'carpenter',\n","  'sequence': 'the man worked as a carpenter.'},\n"," {'score': 0.05238306522369385,\n","  'token': 15610,\n","  'token_str': 'waiter',\n","  'sequence': 'the man worked as a waiter.'},\n"," {'score': 0.04962717741727829,\n","  'token': 13362,\n","  'token_str': 'barber',\n","  'sequence': 'the man worked as a barber.'},\n"," {'score': 0.03788601607084274,\n","  'token': 15893,\n","  'token_str': 'mechanic',\n","  'sequence': 'the man worked as a mechanic.'},\n"," {'score': 0.0376807376742363,\n","  'token': 18968,\n","  'token_str': 'salesman',\n","  'sequence': 'the man worked as a salesman.'}]"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# Men at work\n","unmasker(\"The man worked as a [MASK].\")"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["[{'score': 0.21981723606586456,\n","  'token': 6821,\n","  'token_str': 'nurse',\n","  'sequence': 'the woman worked as a nurse.'},\n"," {'score': 0.15974149107933044,\n","  'token': 13877,\n","  'token_str': 'waitress',\n","  'sequence': 'the woman worked as a waitress.'},\n"," {'score': 0.11547167599201202,\n","  'token': 10850,\n","  'token_str': 'maid',\n","  'sequence': 'the woman worked as a maid.'},\n"," {'score': 0.03796853497624397,\n","  'token': 19215,\n","  'token_str': 'prostitute',\n","  'sequence': 'the woman worked as a prostitute.'},\n"," {'score': 0.03042353130877018,\n","  'token': 5660,\n","  'token_str': 'cook',\n","  'sequence': 'the woman worked as a cook.'}]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# Women at work\n","unmasker(\"The woman worked as a [MASK].\")"]},{"cell_type":"markdown","metadata":{},"source":["What do you notice about the top five responses for men and women? Explain."]},{"cell_type":"markdown","metadata":{},"source":["## Summary\n","\n","We were introduced to using `transformers` in Python with the BERT pretrained model of `bert-base-uncased`."]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":4}
